{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.005148Z",
     "start_time": "2018-10-24T15:49:47.782171Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import pylab as pl\n",
    "import shapely\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.014386Z",
     "start_time": "2018-10-24T15:49:49.008547Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#sys.path.append(\"/home/cusp/fbianco/choroplethNYC/\")\n",
    "import choroplethNYC as cp\n",
    "#from importlib import reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.027828Z",
     "start_time": "2018-10-24T15:49:49.018369Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/fbb/Dropbox//UI/PUIdata'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"PUIDATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## This notebook is designed to pratice with both geopandas and census data.  This is a simple spatial exploratory analysis of census data. The goal is to assess wheather the location of the [linkNYC](https://www.link.nyc/) access points is optimal to guarantee a more \"democratic\" access to the internet.\n",
    "\n",
    "## THE CENSUS: \n",
    "make sure you do the assigned reading about the census. Census data are a national trasure! The census is a survey  designed to collect data on every person that lives in the USA every 10 years. The Census Bureau collects data, designes aggregation areas (the census blocks and tracts for example, which are designed for maximal homogeneity across all features). However, the Census Bureau does not only collect and aggregate data every 10 years, it also collects the America Community Survey every 5 years, and some more restricted survey on an annual bases. The 1 year survey data is what we will use: https://www.census.gov/services/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 1. PUMA : Public Use Microdata Area\n",
    "Census geographies that are not specific political entities (i.e. states, counties etc) are designed for homogeneity so that aggregating the data over the whole area will leat to an estimat (mean or median for example) with minimal variance. Among these geographies are the _census tracts_, _census blocks_, and _Public Use Microdata Areas (PUMAs)_. PUMAs are geographical areas designed to aggregate census data. Each PUMA contains at least 100,000 people. PUMAs do not overlap, and are contained within a single state. \n",
    "\n",
    "## 1.1 download the NYC  Public Use Microdata Areas (PUMA) geometry fron the NYC Open Data API and read it in with geopandas\n",
    "\n",
    "https://data.cityofnewyork.us/Housing-Development/Public-Use-Microdata-Areas-PUMA-/cwiz-gcty/data\n",
    "\n",
    "download it as a shape file. When you download a shapefile you actually download a zipped folder which contains the shape file and other files that are necessary to read the shape file in. You can do that with the urllib library in python 3 with the function \n",
    "\n",
    "        urllib.request.urlretrieve(url, \"file.gz\")\n",
    "\n",
    "then unpack the data into your PUIdata directory, then read it in with geopandas, reading in the shape file with the function \n",
    "\n",
    "        geopandas.GeoDataFrame.from_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "PUMAs were first created for the 1990 Census. PUMAs are identified by a specific 4 numbers id. In the shapefile foung at data.cityofnewyork.us the id is named \"puma\" and it is in fact a 4 digits number. You can read it in as an integer (although of course it is a categorical variable inherently!). Often the PUMA id is found in conjunction with the 3 numbers state id, leading to a 7 numbers identification. The id for the State of NY is 036. See this link for more: https://www.census.gov/geo/reference/geoidentifiers.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.600292Z",
     "start_time": "2018-10-24T15:49:49.030381Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('file.gz', <http.client.HTTPMessage at 0x114ad52e8>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://data.cityofnewyork.us/api/geospatial/cwiz-gcty?method=export&format=Shapefile\"\n",
    "urllib.request.urlretrieve(url, \"file.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.757086Z",
     "start_time": "2018-10-24T15:49:49.602389Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  file.gz\r\n",
      "  inflating: /Users/fbb/Dropbox//UI/PUIdata/geo_export_867ed9e0-8a34-4796-966d-e7e6b1cd6f7e.dbf  \r\n",
      "  inflating: /Users/fbb/Dropbox//UI/PUIdata/geo_export_867ed9e0-8a34-4796-966d-e7e6b1cd6f7e.shp  \r\n",
      "  inflating: /Users/fbb/Dropbox//UI/PUIdata/geo_export_867ed9e0-8a34-4796-966d-e7e6b1cd6f7e.shx  \r\n",
      "  inflating: /Users/fbb/Dropbox//UI/PUIdata/geo_export_867ed9e0-8a34-4796-966d-e7e6b1cd6f7e.prj  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -d $PUIDATA file.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.887327Z",
     "start_time": "2018-10-24T15:49:49.760671Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no such file or directory: '/Users/fbb/Dropbox//UI/PUIdata/geo_export_.shp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-34bb310a6f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pumashp = gpd.GeoDataFrame.from_file(os.getenv(\"PUIDATA\") + \"/\" + \n\u001b[0;32m----> 2\u001b[0;31m                                      \"geo_export_.shp\")\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#                    \"geo_export_4777e173-f310-449c-9fca-1b10b8d6b35a.shp\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/GEOP/lib/python3.6/site-packages/geopandas/geodataframe.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/GEOP/lib/python3.6/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bbox'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/GEOP/lib/python3.6/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no such archive file: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no such file or directory: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[1;32m    164\u001b[0m                        \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: no such file or directory: '/Users/fbb/Dropbox//UI/PUIdata/geo_export_.shp'"
     ]
    }
   ],
   "source": [
    "pumashp = gpd.GeoDataFrame.from_file(os.getenv(\"PUIDATA\") + \"/\" + \n",
    "                                     \"geo_export_.shp\")\n",
    "#                    \"geo_export_4777e173-f310-449c-9fca-1b10b8d6b35a.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.888518Z",
     "start_time": "2018-10-24T15:49:47.810Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.889642Z",
     "start_time": "2018-10-24T15:49:47.816Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashp.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 1.2  plot the PUMA NYC regions\n",
    "\n",
    "you can use the geopandas function plot. \n",
    "What is the appropriate plot to just show the shape of the PUMA regions? A choropleth could be ok, but it is better to just draw the contours of the region, since we do not want to highlight some regions over others by color choices. \n",
    "\n",
    "(Note: a while ago I wrote a quick function that plots choropleths and maps of NYC specifically putting legends and colorbars in the empty spaces taking advantage of the shape of the city and you are welcome to use it: https://github.com/fedhere/choroplethNYC.)\n",
    "\n",
    "Your map should look someting like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.891456Z",
     "start_time": "2018-10-24T15:49:47.823Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://data.cityofnewyork.us/api/geospatial/cwiz-gcty?method=export&format=Shapefile\"\n",
    "urllib.request.urlretrieve(url, \"file.gz\")\n",
    "pumashp.puma = pumashp.puma.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.892911Z",
     "start_time": "2018-10-24T15:49:47.828Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# using the built in plot function as pumashp.plot(pumashp....)\n",
    "# using a choroplethNYC function that FBB wrote\n",
    "cp.choroplethNYC(pumashp, column=None, color=\"white\", edgecolor=\"black\", lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 1: The NYC Public Use Microdata Areas (PUMAs). NYC is divided in 55 PUMAs ranging in size reflecting the population density variations in NYC (since PUMAs are constructed to collect a fix number of inhabitants)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 2. American Fact Finder data on percentage of houses with broadband internet access\n",
    "\n",
    "Download the table of data from 2016 for NYC: you want to obtain data on broadband access (percentage of households with broadband access) at the PUMA (Public Use Microdata Area) geographical area level. \n",
    "\n",
    "This may be tricky. But familiarizing with Census and American Commjunity Survey data is super important for urban science. Here you should use the API, but you should also practice interacting with the website: \n",
    "\n",
    "You  should read the datta in thgouth the API but also download the data manually and compare the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 2.1 Get data with American Fact Finder (AFF) API: \n",
    "you can request an API key, although I think it is not necessary for this search (but it is good to practice). Obtain a key and save it into a python file. DO NOT UPLOAD THE FILE CONTAINING THE API KEY TO GITHUB. Keep your API keys private. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.895129Z",
     "start_time": "2018-10-24T15:49:47.835Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from censusAPI import myAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The internet subscription by household is data surveyd annually. You can find out what are the features that you can query through the API for the annually surveyed data at this url: https://api.census.gov/data/2016/acs/acs1/variables.json\n",
    "\n",
    "read it in as a jason file - you can do it with pandas, but it is slow. You can do it as I do below with the request package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.896796Z",
     "start_time": "2018-10-24T15:49:47.840Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#read in in the variables available. the info you need is in the 1year ACS data\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1/variables.json\"\n",
    "resp = requests.request('GET', url)\n",
    "aff1y = json.loads(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.898895Z",
     "start_time": "2018-10-24T15:49:47.844Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#turning things into arrays to enable broadcasting\n",
    "#Python3\n",
    "affkeys = np.array(list(aff1y['variables'].keys()))\n",
    "#Python2\n",
    "#affkeys = np.array(aff1y['variables'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "You need two features: **total number of households**, and **number of households with broadband access**\n",
    "\n",
    "Broadband access is one of the B28002 keys. However there are many of those! Each variable has several observations associated to it: B28002 is all internet access data (broadband, dial up, none, ...), and for each of those you have the estimate (count, mean, median, percentage...), the margin of errors, and annotations. Those are identified by an appendix that begins with \"\\_\". Look carefully at the line of code below and its output. I am creating a list that contains the keys of the dictionary I created from the json file for all B28002 observations  (rows that start with \"B28002\") that include the word \"Broadband\" in the description.\n",
    "\n",
    "The syntax is a lost comprehension:\n",
    "\n",
    "        as = \\[a for a in listOfAs\\] \n",
    "\n",
    "is simply a compact way to write\n",
    "\n",
    "        as = \\[\\]\n",
    "        for a in listOfAs:\n",
    "            as.append(a)\n",
    "            \n",
    "with an if statement tha selects broadband         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.901056Z",
     "start_time": "2018-10-24T15:49:47.852Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#extracting variables of B28002 that contain \"broadband\"\n",
    "[(k, aff1y['variables'][k]['label'])  for k in affkeys if k.startswith (\"B28002\") and \n",
    " 'Broadband' in aff1y['variables'][k]['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "chose the appropriate variable and extract the relevant data: below I select the variable containing the number of household per PUMA. The \"all\" variable, which tells you what is the total number of units queried (households here) is generally stored in the \\_001 variable (B28002_001 in this case). \"E\" stands for *estimate*. M stands for *margin of error*, EA *estimate annotations*, MA *margin of error annotations*. You want the estimate for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.902643Z",
     "start_time": "2018-10-24T15:49:47.859Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#keyword for the number of households\n",
    "keyNhouseholds = 'B28002_001E'\n",
    "aff1y['variables'][keyNhouseholds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Similarly, choose the fractions of houses with any \"With an Internet subscription!!Broadband of any type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.904126Z",
     "start_time": "2018-10-24T15:49:47.864Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "keyNBB = 'B28002_004E'\n",
    "aff1y['variables'][keyNBB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now that you have the variable names use them to extract the relevant data with the ACS API. \n",
    "\n",
    "The API url is as follows:\n",
    "\n",
    "root: https://api.census.gov/data/2016/acs/acs1\n",
    "\n",
    "action: get=\\[variable Name\\],NAME\n",
    "\n",
    "geometry: for=\\[geometry\\]:\\[desired geometry values\\]in=\\[larger geometry\\]:\\[desired larger geometry values\\]\n",
    "\n",
    "API key: key:\\[api key\\]\n",
    "\n",
    "the URL is constructed as root?action&geometry&key\n",
    "\n",
    "Note that it took me a long time to figure out how to request the right geometry: in the url I write below the geometry is \"public%20use%20microdata%20area\" where %20 is the character for ' ' (space) in a url, and \":\\*\" means all PUMAS\n",
    "\n",
    "state:36 is New York State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.906574Z",
     "start_time": "2018-10-24T15:49:47.869Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# delete\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=\" + keyNBB +\\\n",
    "\",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaBB = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))\n",
    "\n",
    "#getting the first row of the table: the total number of households\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=\"  + keyNhouseholds +\\\n",
    "\",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaPP = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))\n",
    "\n",
    "pumaPP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.908674Z",
     "start_time": "2018-10-24T15:49:47.883Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# getting the broadband access number of households\n",
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=\" + keyNBB +\\\n",
    "\",NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaBB = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))\n",
    "\n",
    "pumaBB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 2.2 generate a feature for the percentage of households with broadband access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.910142Z",
     "start_time": "2018-10-24T15:49:47.892Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#merge number of households w broadband access and total number of households\n",
    "pumaBB = pumaBB.merge(pumaPP).drop(\"Unnamed: 4\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.912248Z",
     "start_time": "2018-10-24T15:49:47.898Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#percentage of households w broadband\n",
    "pumaBB['pcBB'] = pumaBB.B28002_004E.astype(float) / pumaBB.B28002_001E * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.913988Z",
     "start_time": "2018-10-24T15:49:47.905Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumaBB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 2.3 Now compare it with the AFF published \"percentage of households with broadband access\" which is a feature available through their website but that cannot be downloaded directly from the API. It's compiled by AFF the same way we did above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Manually you can download the GCT Geographic Comparison Tables. Download the GCT table that contains the percentage of households in each PUMA with broadband internet subscription as follows:\n",
    "\n",
    "https://factfinder.census.gov/ -> Advanced Search -> Show me all \n",
    "\n",
    "    - Topics: Product Type -> Geography Comparison Table \n",
    "    \n",
    "select the PERCENT OF HOUSEHOLDS WITH A BROADBAND INTERNET SUBSCRIPTION  at our geography granularity (PUMA) and click Download below. This will generate the table on the fly and you can click again on Download.\n",
    "\n",
    "Move the file into the PUIdata directory, unzip it and load it with pandas.\n",
    "    \n",
    "Your table shold be labeled as: GCT2801\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "One more hurdle: you need to move this data to jupyterhub/compute, and the link is not something you can copy and paste!\n",
    "Two solutions:\n",
    "\n",
    "-easier but less preferible: upload the table you downloaded locally to github AWS, or else where, and download it from compute\n",
    "\n",
    "-otherwise, learn how to do it with sftp from your local terminal: these are the steps\n",
    "\n",
    "1. ON YOUR LOCAL MACHINE where you downloaded the table (which was downloaded as aff_download.zip for me in my ~/Downloads folder) type the following *sftp* command\n",
    "        \n",
    "        fbianco@Federicas-MacBook-Air:~$ sftp fbianco@staging.cusp.nyu.edu\n",
    "    \n",
    "2. Input your password\n",
    "            \n",
    "        Password: \n",
    "        Connected to staging.cusp.nyu.edu.\n",
    "\n",
    "   This should open an sftp promopt (you should see sftp> at the beginning of the line). Go to the PUI directory (for me /home/fbianco/PUIdata)\n",
    "    \n",
    "        sftp> cd /home/fbianco/PUIdata             \n",
    "        \n",
    "3. Use the sftp command _put_ to copy the file from the local to the remote machine to compute, making sure you use the full path (unless the file was downloaded in your local directory in the local machine)\n",
    "    \n",
    "        sftp> put /Users/fbianco/Downloads/aff_download.zip\n",
    "        \n",
    "You should see:\n",
    "\n",
    "            Uploading /Users/fbianco/Downloads/aff_download.zip to ....\n",
    "\n",
    "And the file can be unzipped and read into your code with pandas. I leave the read in line below for your convenience, those are the only variables you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.922456Z",
     "start_time": "2018-10-24T15:49:47.915Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# delete\n",
    "!unzip   -d $PUIDATA $PUIDATA/aff_download.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.926460Z",
     "start_time": "2018-10-24T15:49:47.923Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bbpc = pd.read_csv(os.getenv(\"PUIDATA\") + \"/ACS_16_1YR_GCT2801.ST50_with_ann.csv\",\n",
    "            usecols=[\"GCT_STUB.target-geo-id2\",\"HC01\",\"HC02\"])\n",
    "\n",
    "bbpc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "notice 2 things:\n",
    "    - that the Target Geo Id2: GCT_STUB.target-geo-id2 is a 7 digits number. See my comment above about it. \n",
    "    - that there is a weird double header. You can deal with it in one of 2 ways: either reread the file in skipping one row and using the second row as header, or remove that row (for example with bbpc.drop(0, inplace=True)) but also then you should check the *type* of your GCT_STUB.target-geo-id2 feature! The fact that pandas had to read in a column with nombers and characters forced it to read it as a string, and you need to convert it (.astype(int)) to merge it easily with the API acquired data.\n",
    "    \n",
    "In the API dataframe the PUMA id was a 4 digit number. If you have them both as integers and you remove the initial three digits (for example by subtracting 360000 from each value wich you can do as bbpc[\"gid\"] = bbpc.gid - 3600000 then you can merge on the puma id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.930269Z",
     "start_time": "2018-10-24T15:49:47.938Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bbpc.rename(columns={\"GCT_STUB.target-geo-id2\":\"gid\"}, inplace=True)\n",
    "bbpc.drop(0, inplace=True)\n",
    "bbpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.931916Z",
     "start_time": "2018-10-24T15:49:47.959Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bbpc.gid = bbpc.gid.astype(int)\n",
    "bbpc = bbpc[(bbpc.gid > 3600000) & (bbpc.gid < 5700000)]\n",
    "bbpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.935614Z",
     "start_time": "2018-10-24T15:49:47.967Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bbpc[\"gid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.937352Z",
     "start_time": "2018-10-24T15:49:47.973Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bbpc[\"gid\"] = bbpc.gid - 3600000\n",
    "bbpc[\"HC01\"] = bbpc[\"HC01\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.940340Z",
     "start_time": "2018-10-24T15:49:47.978Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bbpc[\"gid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## check that the percentage of households with broadband you generated and the one you red in from the table you downloaded manually are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.941841Z",
     "start_time": "2018-10-24T15:49:47.988Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "diff_ = pumaBB.merge(bbpc, right_on=\"gid\", \n",
    "                     left_on=\"public use microdata area\")[[\"pcBB\", \"HC01\"]]\n",
    "\n",
    "diff_[\"diff\"] = np.abs(diff_[\"pcBB\"] - diff_[\"HC01\"].astype(float))\n",
    "diff_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.949979Z",
     "start_time": "2018-10-24T15:49:47.998Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumaBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.953668Z",
     "start_time": "2018-10-24T15:49:48.006Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumaBB['public use microdata area'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 3 Plot a choropleth of NYC broadband access \n",
    "## 3.1 Merge with the puma geodataframe and plot a choropleth of the percentage of households with broadband access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.955191Z",
     "start_time": "2018-10-24T15:49:48.017Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#choropleth of the percentage of internet axcess\n",
    "pumashp = pumashp.merge(pumaBB, left_on=\"puma\", \n",
    "                   right_on=\"public use microdata area\")\n",
    "\n",
    "cp.choroplethNYC(pumashp,\n",
    "                 column=\"pcBB\", #scheme='equal_interval', k=4,\n",
    "                                   #legend=True, \n",
    "                 cmap=\"viridis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 2: Percentage of households with broadband access (of any kind) at the PUMA granularity level. Percentages range from 55 to >95%, however the only PUMA with less than ~65% households with broadband access is the lower east side. This is a known statistical outliers in NYC on many, if not most socioeconomic features, and it is the site of a large complex of housing projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 4. LinkNYC: assess whether the locations of the linkNYC stations are supplying internet where it is needed.\n",
    "\n",
    "acquire the linkNYC locations and prepare them into a dataframe\n",
    "read it in from the HW9_fb55 folder in :\n",
    "https://github.com/fedhere/PUI2017_fb55\n",
    "\n",
    "Notice that you can also get a linkNYC locations shapefile from NYC open data, as I did to generate this, but I want you do to do some extra coordinates gymnastics for practice so use the one I provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.960584Z",
     "start_time": "2018-10-24T15:49:48.023Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "linkNYC = pd.read_csv(\"https://raw.githubusercontent.com/fedhere/PUI2018_fb55/master/HW7_fb55/linkNYClocations.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.977804Z",
     "start_time": "2018-10-24T15:49:48.030Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## combine long lat into a column like you did in the lab to greate a \"geometry\" column for the dataframe, then convert the dataframe into a GeoDataFrame _linkNYC_ and set native coordinates  frame to lat/lon as you did in the lab\n",
    "    linkNYC.crs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.981033Z",
     "start_time": "2018-10-24T15:49:48.036Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# combine lat and lon to one column\n",
    "linkNYC['lonlat'] = list(zip(linkNYC.longitude, linkNYC.latitude))\n",
    "# Create Point Geometry for based on lonlat column\n",
    "linkNYC['geometry'] = linkNYC[['lonlat']].applymap(lambda x:shapely.geometry.Point(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.984144Z",
     "start_time": "2018-10-24T15:49:48.044Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "linkNYC = gpd.GeoDataFrame(linkNYC)\n",
    "linkNYC.crs = from_epsg(4326) # epsg=4326: lat/on | 26918: NAD83/UTM zone 18N | epsg=2263 is US feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.989181Z",
     "start_time": "2018-10-24T15:49:48.055Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "linkNYC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# plot the location of the linkNYC stations on top of a choropleth of broadband access percentage in *5 equal intervals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.990828Z",
     "start_time": "2018-10-24T15:49:48.066Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "linkNYC.date_link_ = pd.to_datetime(linkNYC.date_link_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.992924Z",
     "start_time": "2018-10-24T15:49:48.078Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "type(linkNYC.date_link_.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:49.996669Z",
     "start_time": "2018-10-24T15:49:48.085Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ax = pl.figure(figsize=(10,10)).add_subplot(111)\n",
    "cp.choroplethNYC(pumashp,\n",
    "                 column=\"pcBB\", ax=ax, kind=\"discrete\", k=2, #scheme='equal_interval', k=4,\n",
    "                                   #legend=True, \n",
    "                 cmap=\"bone\");\n",
    "#cp.choroplethNYC(pumashp, column=\"pcBB\", scheme='equal_interval', k=5,\n",
    "#             cmap=\"bone\", ax=ax, edgecolor=\"black\", cb=True)\n",
    "#linkNYC.plot(ax=ax, marker='o', c=\"date_link_\", \n",
    "#             markersize=5, cmap=\"OrRd\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.000782Z",
     "start_time": "2018-10-24T15:49:48.092Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "p = pumashp.plot(column=\"pcBB\", scheme='equal_interval',ax=ax, k=5, edgecolor=\"black\", cmap='bone')\n",
    "linkNYC.plot(ax=ax, marker='o', c=\"date_link_\", \n",
    "             markersize=5, cmap=\"OrRd\");\n",
    "norm = Normalize(vmin=pumashp.pcBB.min(), vmax=pumashp.pcBB.max())\n",
    "n_cmap = cm.ScalarMappable(norm=norm, cmap='bone')\n",
    "n_cmap.set_array([])\n",
    "ax.get_figure().colorbar(n_cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.003606Z",
     "start_time": "2018-10-24T15:49:48.098Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashp.plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 3: Location of linkNYC hubs (dots) overplotted on a map of NYC color-coded to reflect the percentage of houses with broadband internet acces at the PUMA level, in 5 equal intervals (darker indicating a smaller percentage of hoses with broadband access). The color of the dots corresponding to the linkNYC dots represents the age of the hub, lighter colors (yellow) indicating older hubs, darker colors (red) indicating more recent hubs, from the earliest linkNYC location placed on 2015-12-28, to the latest ones at the time the data were gathered, placed on 2017-11-04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# 5 Find the number of linkNYC locations per person by PUMA\n",
    "\n",
    "## 5.1 with the AFF API from ACS get the total population by puma and merge it into your GeoDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "the ACS population by  variable is B00001_001E, and of course your geometry is PUMA, _public%20use%20microdata%20area:*_, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.006594Z",
     "start_time": "2018-10-24T15:49:48.107Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://api.census.gov/data/2016/acs/acs1?get=B00001_001E,\" +\\\n",
    "\"NAME&for=public%20use%20microdata%20area:*&in=state:36&key=\" + myAPI\n",
    "resp = requests.request('GET', url).content\n",
    "pumaPop = pd.read_csv(io.StringIO(resp.decode('utf-8').replace('[','').replace(']','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.009985Z",
     "start_time": "2018-10-24T15:49:48.115Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumaPop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.012081Z",
     "start_time": "2018-10-24T15:49:48.124Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashp = pumashp.merge(pumaPop, left_on=\"puma\", right_on=\"public use microdata area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.015381Z",
     "start_time": "2018-10-24T15:49:48.133Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 5.2 find the number of station per 100 people per PUMA\n",
    "**Important** you can do this with an sjoin(). But sjoin() should not be used with lat/lon coordinates cause they are **not \"flat coordinates\"**. Since spattial joins are done in cartesian geometry the only coordinate systems suitable to spatial joins are flat coordinate plane systems, which in the NYC area is 2263. So before you proceed to the sjoin you have to convert both pumashp and linkNYC to 2263 (for example with GeoDataFrame method .to_crs(epsg=...) )\n",
    "\n",
    "\n",
    "(A note: You can also do this by hand by asking for each point if it is in any of the regions, for example in a for loop. But that is escruciatingly slow, unless you get smart about your requests (for example for a given PUMA not asking if the coordinates of a point are very different from the center of a PUMA). With the new version of geopandas this is not needed, cause the sjoin is fast, but with the older versions this was sometimes better) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "convert coordinates for pumashp and linkNYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.016904Z",
     "start_time": "2018-10-24T15:49:48.141Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#set coordinates to flat plane\n",
    "pumashp = pumashp.to_crs(epsg=2263) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.019276Z",
     "start_time": "2018-10-24T15:49:48.149Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#set coordinates to flat plane\n",
    "linkNYC = linkNYC.to_crs(epsg=2263) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "create a geodataframe with a spatial join and use groupby to count the number of linkNYC in each PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.024458Z",
     "start_time": "2018-10-24T15:49:48.157Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#spatial join to \"within\"\n",
    "linkpp = gpd.sjoin(linkNYC, pumashp)[[\"puma\", \"link_site\"]].groupby(\"puma\").count()\n",
    "linkpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "finally merge back into the pumashp. Make sure you use the correct scheme for merging: you want to have _all_ the PUMAs, not only the ones that have linkNYC in them in the final geoDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.027294Z",
     "start_time": "2018-10-24T15:49:48.167Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#merge outer so as to not to lose pumas\n",
    "pumashplc = pumashp.merge(linkpp, left_on=\"puma\", right_index=True, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.032239Z",
     "start_time": "2018-10-24T15:49:48.179Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashplc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.035469Z",
     "start_time": "2018-10-24T15:49:48.190Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#linkNYC per 100 inhabitants\n",
    "pumashplc[\"linkNYCpp\"] = pumashplc[\"link_site\"] / pumashplc[\"B00001_001E\"] * 100\n",
    "pumashplc[\"linkNYCpp\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# at last, plot the linkNYC locations on top of a choropleth of number of stations per 100 people  in 10 equal intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.039427Z",
     "start_time": "2018-10-24T15:49:48.200Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#plotting (plotting map underneath main choropleth cause i have NaNs\n",
    "f, ax = cp.choroplethNYC(pumashp, column=None, color=\"white\", edgecolor=\"black\")\n",
    "f, ax, c = cp.choroplethNYC(pumashplc, column=\"linkNYCpp\", scheme='equal_interval', k=10,\n",
    "                                    cmap=\"bone\", ax=ax, edgecolor=\"white\", lw=0.2)\n",
    "linkNYC.plot(marker='o', c=\"date_link_\", \n",
    "             markersize=5, cmap=\"OrRd\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 4: Map of linkNYC locations (dots as in Fig. 3) and number of linkNYC per 100 inhabitants, in equal intervals as indicated in the caption. The high concentration of linkNYC hubs in Manhattan is reflected by the brighter color of the PUMAs in this area: despite generally being the most populated areas, the midtown and lower Manhattan pumas have the highest number of linkNYC per inhabitant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Extra credit 1: \n",
    "How would you enhance the visibility of this map? you are working with numbers that are mostly very low, and reside near each other, and them have a few much higher value points. Notice that here the high values are interesting though and should not be thrown away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.047800Z",
     "start_time": "2018-10-24T15:49:48.206Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashplc.linkNYCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.050256Z",
     "start_time": "2018-10-24T15:49:48.215Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashplc[\"loglinkNYCpp\"] = np.log10(pumashplc[\"linkNYCpp\"])\n",
    "pumashplc[\"loglinkNYCpp\"][~np.isfinite(pumashplc[\"loglinkNYCpp\"])] = \\\n",
    "        pumashplc[\"loglinkNYCpp\"][np.isfinite(pumashplc[\"loglinkNYCpp\"])].min() - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.052806Z",
     "start_time": "2018-10-24T15:49:48.223Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashplc.linkNYCpp[np.isnan(pumashplc.linkNYCpp)] = np.nanmin(pumashplc.linkNYCpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.054508Z",
     "start_time": "2018-10-24T15:49:48.231Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ax = pl.figure().add_subplot(111)\n",
    "ax.hist(pumashplc.linkNYCpp, bins=30);\n",
    "ax.set_xlabel(\"linkNYC hubs per 100 people \")\n",
    "ax.set_ylabel(\"number of PUMAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 5: Number of linkNYC hubs per 100 people by PUMA. Most PUMAs have none, or anyhow less than 2  hubs per 100 people, with a maximum of 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.058210Z",
     "start_time": "2018-10-24T15:49:48.242Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashplc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.059566Z",
     "start_time": "2018-10-24T15:49:48.249Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#plotting (plotting map underneath main choropleth cause i have NaNs\n",
    "f, ax = cp.choroplethNYC(pumashp, column=None, color=\"white\", edgecolor=\"black\")\n",
    "f, ax, c = cp.choroplethNYC(pumashplc, column=\"loglinkNYCpp\",\n",
    "                                    cmap=\"bone\", ax=ax, edgecolor=\"white\", lw=0.2)\n",
    "linkNYC.plot(marker='o', c=\"date_link_\", \n",
    "             markersize=5, cmap=\"OrRd\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 6: As Fig 4, but with the number of linkNYC per person plotted in a logaritmic scale: this scaling allows to isolate PUMA with no linkNYC hubs from PUMAs with few linkNYC hubs per person and reduces the gradient allowing to see differences in the linkNYC per inhabitant in PUMAs in the outer borroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Extra Credit 2:\n",
    "plot Spatial Lag Deciles that describe the impact of linkNYC in providing access where needed. You can follow what you did in the lab, but you need to create a metric based on the data you have tha measures the impact of linkNYC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## in order to account for neighbouring PUMAs with stations create a spatial weights matrix to assess neighbour distances for the PUMA geometries - we use queens weights. For theoretical details on this see:\n",
    "# [Anselin Rey & Li 2014](http://www.tandfonline.com/doi/full/10.1080/13658816.2014.917313?scroll=top&needAccess=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.062520Z",
     "start_time": "2018-10-24T15:49:48.261Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pysal as ps\n",
    "qW_CT = ps.weights.Queen.from_dataframe(pumashp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.064827Z",
     "start_time": "2018-10-24T15:49:48.292Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#cardinalities of the 10th puma\n",
    "for i in 1, 10, 20, 30:\n",
    "    ax = pumashp[i:i+1].plot()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ax = pumashp.plot(color=\"None\",ax=ax, linewidth=2, edgecolor=\"k\")\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_title(\"cardinalities %d\"%qW_CT.cardinalities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.067721Z",
     "start_time": "2018-10-24T15:49:48.300Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# the weight matrix\n",
    "Wmatrix, ids = qW_CT.full()\n",
    "Wmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.070560Z",
     "start_time": "2018-10-24T15:49:48.309Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "qW_CT.transform = 'r'\n",
    "pumashplc[\"linkNYCpponBBpercentagesq\"] = (pumashplc[\"linkNYCpp\"] / \n",
    "                                          (pumashplc.pcBB / 100))\n",
    "linkedNYCLag = ps.lag_spatial(qW_CT, pumashplc[\"linkNYCpponBBpercentagesq\"])\n",
    "\n",
    "linkedNYCpc = ps.Percentiles(linkedNYCLag.squeeze(), [0,25,50,75,100])\n",
    "linkedNYCpc.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.072484Z",
     "start_time": "2018-10-24T15:49:48.317Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pumashplc[\"sqlinkNYCpponBBpercentagesq\"] = (np.sqrt(pumashplc[\"linkNYCpp\"])**0.5 / \n",
    "                                            (pumashplc.pcBB / 100)**2)\n",
    "linkedNYCLag2 = ps.lag_spatial(qW_CT, pumashplc[\"sqlinkNYCpponBBpercentagesq\"])\n",
    "linkedNYCpc2 = ps.Percentiles(linkedNYCLag2.squeeze(), [0,25,50,75,100])\n",
    "linkedNYCpc2.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:49:50.075392Z",
     "start_time": "2018-10-24T15:49:48.325Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#f, ax = plt.subplots(figsize=(9, 9))\n",
    "f = pl.figure(figsize=(10, 5))\n",
    "ax = f.add_subplot(121)\n",
    "pumashp.plot(color='white', edgecolor='black', ax=ax, lw=0.1)\n",
    "pumashplc.assign(cl=linkedNYCpc.yb).plot(column='cl', categorical=True, \n",
    "        k=5, cmap='hot', linewidth=0.1, ax=ax, \n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.title(\"linkNYC per person Spatial Lag Deciles\");\n",
    "ax = f.add_subplot(122)\n",
    "pumashp.plot(color='white', edgecolor='black', ax=ax, lw=0.1)\n",
    "pumashplc.assign(cl=linkedNYCpc2.yb).plot(column='cl', categorical=True, \n",
    "        k=5, cmap='hot', linewidth=0.1, ax=ax, \n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.title(\"linkNYC per person Spatial Lag Deciles\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### Fig. 7: Two metrics of the impact of linkNYC hubs,  constructed one, *on the left*, as the ratio of linkNYC per 100 inhabitants ($N_{linkNYC_{100}}$) over the fraction of houses with broadband access ($f_{BB}$): $ \\frac{N_{linkNYC_{100}}}{f_{BB}}$ and *on the right*  the square root of the linkNYC hubs per 100 people over the square of the  fraction of houses with broadband access. $ \\frac{\\sqrt{N_{linkNYC_{100}}}}{f_{BB}^2}$, giving less weight to the presence of linkNYC hubs and more wight to the BB fraction. Both are aggregated and plotted as  spatial lag deciles (the average of the values for the neighbouring regions) with queen's weights (Anselin, Rey & Li 2014) at the PUMA geographic granularity level.  Small differences are visible in the impact of linkNYC in the Bronx and in Brooklyn,  where the second metric shows a higher impact, and Queens where the impact measured by the second metric is less.   Colors from black through red and yellow to white indicate higher impact of linkNYC as defined above. The area of highest impact are in Manhattan and the Brooklyn PUMAs that face the East river: with this metric the linkNYC aboundance in midtown Manhattan dominates the metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "Note: To find the table you extracted through the API manually you could\n",
    "\n",
    "https://factfinder.census.gov/ -> Advanced Search -> Show me all \n",
    "    \n",
    "    - Topics: Housing -> Physical Characteristics -> Internet Access\n",
    "    - Geographies -> all geography types -> Public Use Microdata Areas (755)\n",
    "                     -> Select a State: New York -> All Pubic Use Microdaa Areas within New York\n",
    "                     -> Add Your Selection\n",
    "    - Years -> 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "415px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
